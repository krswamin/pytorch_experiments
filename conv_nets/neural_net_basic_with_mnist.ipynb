{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29445943-c69d-42e0-98b7-337ca97524ed",
   "metadata": {},
   "source": [
    "# Neural Net\n",
    "Basic neural net with fully connected layers. \\\n",
    "Demo on MNIST dataset. \\\n",
    "Gpu , Datasets, Dateloader, Transforms, Neural Net, Training and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d04b53-9fc4-453c-9a09-151a7613b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper Parameters\n",
    "input_size = 784 #(MNist images are 28*28 = 784)\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "#MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root = '../data',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.ToTensor(),\n",
    "                                           download = True)\n",
    "test_dataset = torchvision.datasets.MNIST(root = '../data',\n",
    "                                           train = False,\n",
    "                                           transform = transforms.ToTensor())\n",
    "    \n",
    "\n",
    "# mnist\n",
    "print(\"train_dataset: details\")\n",
    "print(f'train_dataset size     :{list(train_dataset.train_data.size())}')#  [60000, 28, 28]\n",
    "print(f'train_dataset mean     :{train_dataset.train_data.float().mean()}')\n",
    "print(f'train_dataset mean/255 :{train_dataset.train_data.float().mean()/255}') # 0.1306604762738429\n",
    "print(f'train_dataset std-dev  :{train_dataset.train_data.float().std()}')\n",
    "print(f'train_dataset std-dev/255:{train_dataset.train_data.float().std()/255}') # 0.30810780717887876\n",
    "\n",
    "print(\"\\n train_dataset: details\")\n",
    "print(f'train_dataset size     :{list(train_dataset.train_data.size())}')#  [60000, 28, 28]\n",
    "print(f'train_dataset mean     :{train_dataset.train_data.float().mean(axis = (0,1,2))}')\n",
    "print(f'train_dataset mean/255 :{train_dataset.train_data.float().mean(axis = (0,1,2))/255}') # 0.1306604762738429\n",
    "print(f'train_dataset std-dev  :{train_dataset.train_data.float().std(axis = (0,1,2))}')\n",
    "print(f'train_dataset std-dev/255:{train_dataset.train_data.float().std(axis = (0,1,2))/255}') # 0.30810780717887876\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset= train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle =True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset= test_dataset,\n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle =False)\n",
    "examples = iter(test_loader)\n",
    "\n",
    "example_data , example_targets = next(examples)\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(example_data[i][0], cmap = 'gray')\n",
    "plt.show()\n",
    "\n",
    "print(f'Num_MNIST_samples = {len(train_dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e26823-0ab8-4c84-a818-ce309ea76921",
   "metadata": {},
   "source": [
    "## Fully Connected Neural Network with one hidden layer\n",
    "Note: in PyTorch, when you call an instance of a nn.Module, it always internally calls the forward method. \\\n",
    "\\\n",
    "This behavior is due to the __call__ method defined within the nn.Module class. When you invoke a nn.Module object like a function (e.g., model(input)), the __call__ method is executed, which in turn calls the forward method with the provided input.\\\n",
    "\\\n",
    "It's important to note that you should not directly call the forward method yourself (e.g., model.forward(input)). Calling the module instance ensures that all the necessary hooks and mechanisms within PyTorch are properly executed, including pre-forward hooks, the actual forward pass, and post-forward hooks. Directly calling forward bypasses these mechanisms, potentially leading to unexpected behavior, especially when using features like hooks or when working with models in training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b28b2b-f410-4521-8e76-cb984dfc470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "        # no softmax or activation at the end\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.l2(out)\n",
    "        # no softmax or activation at the end\n",
    "        '''\n",
    "        # This is because : in Pytorch if one uses the nn.CrossEntropyLoss the input must be unnormalized raw value (aka logits), \n",
    "        the target must be class index instead of one hot encoded vectors.. Very very important\n",
    "        Hence there is no need of softmax\n",
    "        '''\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss \n",
    "''' By definition the cross entropy loss uses the output probabliltities of a softmax layer (as model ouput) and a \n",
    "one hot encoded vector( for the label). And the calculation is done\n",
    "\n",
    "In Pytorch this is different though\n",
    "In Pytorch if one uses the nn.CrossEntropyLoss the input must be unnormalized raw value (aka logits), \n",
    "        the target must be class index instead of one hot encoded vectors.. Very very important\n",
    "        Hence there is no need of softmax\n",
    "'''\n",
    "loss_handle = nn.CrossEntropyLoss()\n",
    "# Optimizer for back propagation and weight updates\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "img, label = next(iter(train_loader))\n",
    "print('img.shape   =',img.shape)\n",
    "print('label.shape =',label.shape)\n",
    "train_loader_len = len(train_loader)\n",
    "print(f'Num_MNIST_samples = {train_loader_len}')\n",
    "print('Checking if using the iterator and next reduced the train loader length by one')\n",
    "train_loader_len = len(train_loader)\n",
    "print(f'Num_MNIST_samples = {train_loader_len}')\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape it to the Neural Network input size which is 28*28 = 784\n",
    "        # Deploy the images to the appropriate device\n",
    "        # the train loader loads the number of images as specified by the batch size.\n",
    "        # The batch_size as specified in the beginning is 100. So images size is batch_size * 784 = 100*784\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        # Deploy labels to the device\n",
    "        labels = labels.to(device)\n",
    "        # model_outputs: This model output has 10 values per sample (corresponding to the 10 classes)\n",
    "        # model_outputs size is 100*10 (batch_size * num_classes)\n",
    "        model_outputs =  model(images)\n",
    "        '''in Pytorch if one uses the nn.CrossEntropyLoss the input must be unnormalized raw value (aka logits), \n",
    "        the target must be class index instead of one hot encoded vectors.. Very very important\n",
    "        Hence there is no need of softmax'''\n",
    "        # Loss\n",
    "        loss = loss_handle(model_outputs, labels)\n",
    "        # Back propagate the loss\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        # Zero the gradients , before the next back propagation    \n",
    "        # (why dont the gradients get updated to zero/reset every time backpropagate is called\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{train_loader_len}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        # We would have to choose the max value, the max index rather and that would be indicator of the class\n",
    "        # Each row has two columns. So in each row choose the highest value (and hence the highest index)\n",
    "        max_values, max_indices = torch.max(model_outputs, dim=1)\n",
    "        y_predicteds = max_indices\n",
    "        if i == 0 and epoch == 0:\n",
    "            print(f'images.shape        = {images.shape}')\n",
    "            print(f'labels.shape        = {labels.shape}')\n",
    "            print(f'model_outputs.shape = {model_outputs.shape}')\n",
    "            print(f'max_indices.shape   = {max_indices.shape}')\n",
    "            print(f'y_predicteds.shape  = {y_predicteds.shape}')\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05677bd-4555-4867-b23c-c79263304753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.l2 = nn.Linear(hidden_size, num_classes)\n",
    "        # no softmax or activation at the end\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.l2(out)\n",
    "        # no softmax or activation at the end\n",
    "        '''\n",
    "        # This is because : in Pytorch if one uses the nn.CrossEntropyLoss the input must be unnormalized raw value (aka logits), \n",
    "        the target must be class index instead of one hot encoded vectors.. Very very important\n",
    "        Hence there is no need of softmax\n",
    "        '''\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "print('model.parameters()=', model.parameters())\n",
    "# Loss \n",
    "loss_handle = nn.CrossEntropyLoss()\n",
    "# Optimizer for back propagation and weight updates\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "img, label = next(iter(train_loader))\n",
    "print('img.shape   =',img.shape)\n",
    "print('label.shape =',label.shape)\n",
    "train_loader_len = len(train_loader)\n",
    "print(f'Num_MNIST_samples = {train_loader_len}')\n",
    "print('Checking if using the iterator and next reduced the train loader length by one')\n",
    "train_loader_len = len(train_loader)\n",
    "print(f'Num_MNIST_samples = {train_loader_len}')\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape it to the Neural Network input size which is 28*28 = 784\n",
    "        # Deploy the images to the appropriate device\n",
    "        # the train loader loads the number of images as specified by the batch size.\n",
    "        # The batch_size as specified in the beginning is 100. So images size is batch_size * 784 = 100*784\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        # Deploy labels to the device\n",
    "        labels = labels.to(device)\n",
    "        # model_outputs: This model output has 10 values per sample (corresponding to the 10 classes)\n",
    "        # model_outputs size is 100*10 (batch_size * num_classes)\n",
    "        model_outputs =  model(images)\n",
    "        '''in Pytorch if one uses the nn.CrossEntropyLoss the input must be unnormalized raw value (aka logits), \n",
    "        the target must be class index instead of one hot encoded vectors.. Very very important\n",
    "        Hence there is no need of softmax'''\n",
    "        # Loss\n",
    "        loss = loss_handle(model_outputs, labels)\n",
    "        # Back propagate the loss\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        # Zero the gradients , before the next back propagation    \n",
    "        # (why dont the gradients get updated to zero/reset every time backpropagate is called\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{train_loader_len}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        # We would have to choose the max value, the max index rather and that would be indicator of the class\n",
    "        # Each row has two columns. So in each row choose the highest value (and hence the highest index)\n",
    "        max_values, max_indices = torch.max(model_outputs, dim=1)\n",
    "        y_predicteds = max_indices\n",
    "        if i == 0 and epoch == 0:\n",
    "            print(f'images.shape        = {images.shape}')\n",
    "            print(f'labels.shape        = {labels.shape}')\n",
    "            print(f'model_outputs.shape = {model_outputs.shape}')\n",
    "            print(f'max_indices.shape   = {max_indices.shape}')\n",
    "            print(f'y_predicteds.shape  = {y_predicteds.shape}')\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed9b550-8f51-4c93-81a8-6680ce197b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "## KSW: TODO. This is wrong fix it\n",
    "test_loader_len = len(test_loader)\n",
    "print(f'Num_MNIST_samples = {test_loader_len}')\n",
    "\n",
    "n_correct = 0\n",
    "n_samples = len(test_loader.dataset)\n",
    "\n",
    "for _, (images, labels) in enumerate(test_loader):\n",
    "    images = images.reshape(-1, 28*28)\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    # Y predicted\n",
    "    y_predicteds =  model(images)\n",
    "\n",
    "    print('labels.shape()=',labels.shape)\n",
    "    print('y_predicteds.shape()=',y_predicteds.shape)\n",
    "    \n",
    "    # Loss\n",
    "    loss = loss_handle(y_predicteds, labels)\n",
    "    # Back propagate the loss\n",
    "    loss.backward()\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "    # Zero the gradients , before the next back propagation    \n",
    "    # (why dont the gradients get updated to zero/reset every time backpropagate is called\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    n_correct += (y_predicteds == labels).sum().item()\n",
    "\n",
    "acc = n_correct / n_samples\n",
    "print(f'Accuracy of the network on the {n_samples} test images: {100*acc} %') \n",
    "\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
