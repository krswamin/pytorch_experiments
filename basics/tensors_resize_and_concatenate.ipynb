{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b21e859-9fa7-46c0-8f97-de039a061304",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PyTorch's convolutional layers, such as nn.Conv2d, expect input tensors in the NCHW format. This format represents the dimensions in the following order:\n",
    "N: (Batch size): The number of independent samples or images in a batch.\n",
    "C: (Channels): The number of color channels in the image (e.g., 3 for RGB, 1 for grayscale).\n",
    "H: (Height): The height of the image in pixels.\n",
    "W: (Width): The width of the image in pixels.\n",
    "Therefore, an input image for a 2D convolution in PyTorch should have the shape (N, C, H, W). For example, a batch of 10 RGB images with a height of 256 pixels and a width of 256 pixels would have a shape of (10, 3, 256, 256).\n",
    "'''\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "print(\"\\n\\n----------------------------------------------------------\")\n",
    "print(\"TOPIC 1: DEMONSTRATING WHAT tensor_a.shape[2:] DOES\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "# Create a tensor\n",
    "tensor_a = torch.randn(3, 2, 16, 16) \n",
    "# Get the shape using .shape\n",
    "print(\"Tensor a\")\n",
    "shape_tuple = tensor_a.shape\n",
    "print(f\"Shape using .shape: {shape_tuple}\") #output: Shape using .shape: torch.Size([2, 3, 4])\n",
    "\n",
    "# Get the shape using .size()\n",
    "size_tuple = tensor_a.size()\n",
    "print(f\"Shape using .size(): {size_tuple}\") #output: Shape using .size(): torch.Size([2, 3, 4])\n",
    "\n",
    "print(f\"tensor_a.shape[2:] prints the last two dimensions of the tensor: {tensor_a.shape[2:]}\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n----------------------------------------------------------\")\n",
    "print(\"TOPIC 2: RESIZING A BATCH OF TENSORS THAT ARE IMAGES \")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"torchvision.transforms.functional.resize is specifically for image tensors (there could be other types of resizes in pytorch). \\\n",
    "it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions. \\\n",
    "i.e the last two dimensions are assumed to be H,W and resizing occurs along these dimensions\")\n",
    "# Tensor b\n",
    "tensor_b = torch.randn(3, 5, 10, 10) \n",
    "print(\"\\nTensor b: before resizing \")\n",
    "print(tensor_b.shape)\n",
    "tensor_b= TF.resize(tensor_b,  size=tensor_a.shape[2:])\n",
    "print(\"\\nTensor b: after resizing along the last two dimensions tensor_b.shape[2:]\")\n",
    "print(tensor_b.shape)\n",
    "\n",
    "\n",
    "print(\"\\n\\n----------------------------------------------------------\")\n",
    "print(\"TOPIC 3: CONCATENATING ALONG VARIOUS DIMENSIONS\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"When concatenating along a particular dimension. That dimension can be different. But all other dimensions should be the same. \\\n",
    "For example if you are concatenating along dim1 = channels. Number of channels can be different. \\\n",
    "But dim0=number of batch samples should be the same. dim2,dim3= (H,W) should also be the same\")\n",
    "\n",
    "# Tensor A,B and C1\n",
    "# Pytorch tensors are of the format : (N,C, H, W) = (dim0, dim1, dim2, dim3). \n",
    "# Concate along the number of channels = dimension 1 : 5+5 = 10\n",
    "tensor_c1 = torch.cat((tensor_a, tensor_b), dim=1)\n",
    "print(\"\\nTensor c1: concatenate along num of channels(dimension 2). Example for concatenating skip connections\")\n",
    "print(f\"tensor_a.shape: {tensor_a.shape}\")\n",
    "print(f\"tensor_b.shape: {tensor_b.shape}\")\n",
    "print(f\"tensor_c1.shape: {tensor_c1.shape}\")\n",
    "\n",
    "# Tensor A,D and C2\n",
    "# Pytorch tensors are of the format : (N,C, H, W) = (dim0, dim1, dim2, dim3). \n",
    "# Concate along the height = dimension 2 : 10+10 = 20\n",
    "print(\"\\nTensor c2: concatenate along height(dimension 3). You will never need this LOL. Why will you want to increase the size of the image\")\n",
    "tensor_d = torch.randn(3, 2, 30, 16) \n",
    "tensor_c2 = torch.cat((tensor_a, tensor_d), dim=2)\n",
    "print(f\"tensor_a.shape:  {tensor_a.shape}\")\n",
    "print(f\"tensor_d.shape:  {tensor_d.shape}\")\n",
    "print(f\"tensor_c2.shape: {tensor_c2.shape}\")\n",
    "\n",
    "# Tensor A,E and C3\n",
    "# Pytorch tensors are of the format : (N,C, H, W) = (dim0, dim1, dim2, dim3). \n",
    "# Concate along the height = dimension 2 : 10+10 = 20\n",
    "print(\"\\nTensor c3: concatenate along batche samples(dimension 0). Example when u are increasing the samples in a batch\")\n",
    "tensor_e = torch.randn(105, 2, 16, 16) \n",
    "tensor_c3 = torch.cat((tensor_a, tensor_e), dim=0)\n",
    "print(f\"tensor_a.shape:  {tensor_a.shape}\")\n",
    "print(f\"tensor_e.shape:  {tensor_e.shape}\")\n",
    "print(f\"tensor_c3.shape: {tensor_c3.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
