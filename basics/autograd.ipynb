{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "883d511b-1352-40ab-bdec-0967c8f5fd25",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "useful video: https://www.youtube.com/watch?v=OIenNRt2bjg\n",
    "\n",
    "Neural network training requires differenctiation (during the back propagation in the training)\n",
    "Autograd is pytorch's automatic differentiation engine\n",
    "Computes partial derivatives using chain rule (Vector Jacobian product)\n",
    "\n",
    "also sth about accumulating the gradient and empyting the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92efc53-5b48-49c2-a039-306d0bf6113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32daf9b-f778-4554-b59d-fbd51e366667",
   "metadata": {},
   "source": [
    "## X without grad option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41eae54-c97a-472c-8049-233c91a0dde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ x without grad option --------\n",
      "tensor([ 0.1321, -1.4492, -3.5430])\n",
      "tensor([ 2.1321,  0.5508, -1.5430])\n",
      "None\n",
      "None\n",
      "------ z which is a function of y and hence x--------\n",
      "tensor([13.6369,  0.9102,  7.1426])\n",
      "None\n",
      "-----------\n",
      "tensor(7.2299)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"------ x without grad option --------\")\n",
    "x = torch.randn(3)\n",
    "y = x+2\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.grad_fn)\n",
    "print(y.grad_fn)\n",
    "\n",
    "print(\"------ z which is a function of y and hence x--------\")\n",
    "z = y*y*3\n",
    "print(z)\n",
    "print(z.grad_fn)\n",
    "\n",
    "print(\"-----------\")\n",
    "z = z.mean()\n",
    "print(z)\n",
    "print(z.grad_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17c902c-fd13-48ce-8809-aca4a6c3e240",
   "metadata": {},
   "source": [
    "## X with grad option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f4d94d-3257-46c2-b8b0-bd94620576c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ x with grad option --------\n",
      "tensor([0.9165, 1.7777, 0.7955], requires_grad=True)\n",
      "tensor(3.1632, grad_fn=<MeanBackward0>)\n",
      "None\n",
      "<MeanBackward0 object at 0x78a1526b8a60>\n",
      "------ z which is a function of y and hence x-------\n",
      "tensor(30.0173, grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x78a1526b8a60>\n",
      "-----------\n",
      "tensor(30.0173, grad_fn=<MeanBackward0>)\n",
      "<MeanBackward0 object at 0x78a1526b8a60>\n",
      " ---- y.backward and z.backward----\n",
      "None\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([6.6597, 6.6597, 6.6597])\n"
     ]
    }
   ],
   "source": [
    "print(\"------ x with grad option --------\")\n",
    "# When requires_grad = True , it is tracked on a computational graph\n",
    "x = torch.randn(3, requires_grad = True)\n",
    "y = x+2\n",
    "y = y.mean()\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.grad_fn)\n",
    "print(y.grad_fn)\n",
    "\n",
    "print(\"------ z which is a function of y and hence x-------\")\n",
    "z = y*y*3\n",
    "print(z)\n",
    "print(z.grad_fn)\n",
    "\n",
    "print(\"-----------\")\n",
    "z = z.mean()\n",
    "print(z)\n",
    "print(z.grad_fn)\n",
    "\n",
    "print(\" ---- y.backward and z.backward----\")\n",
    "\n",
    "# Note that you can calculates y and z backward only on scalars , not on vectors. \n",
    "# so y = y.mean() and z = z.mean()\n",
    "\n",
    "print(x.grad)\n",
    "y.backward() # dy/dx\n",
    "print(x.grad)\n",
    "z.backward() # dz/dx\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c60fa97-5eca-4e0e-9218-9dcf2e4dac2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ x with grad option --------\n",
      "tensor([-2.6937, -0.1593,  0.3369], requires_grad=True)\n",
      "tensor(1.1613, grad_fn=<MeanBackward0>)\n",
      "None\n",
      "<MeanBackward0 object at 0x78a1529132e0>\n",
      "\n",
      "------ z which is a function of y and hence x-------\n",
      "tensor(4.0459, grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x78a1529132e0>\n",
      "\n",
      "-----------\n",
      "tensor(4.0459, grad_fn=<MeanBackward0>)\n",
      "<MeanBackward0 object at 0x78a1529132e0>\n",
      "\n",
      " ---- z.backward-----\n",
      "None\n",
      "tensor([2.3226, 2.3226, 2.3226])\n"
     ]
    }
   ],
   "source": [
    "print(\"------ x with grad option --------\")\n",
    "# When requires_grad = True , it is tracked on a computational graph\n",
    "x = torch.randn(3, requires_grad = True)\n",
    "y = x+2\n",
    "y = y.mean()\n",
    "print(x)\n",
    "print(y)\n",
    "print(x.grad_fn)\n",
    "print(y.grad_fn)\n",
    "\n",
    "print(\"\\n------ z which is a function of y and hence x-------\")\n",
    "z = y*y*3\n",
    "print(z)\n",
    "print(z.grad_fn)\n",
    "\n",
    "print(\"\\n-----------\")\n",
    "z = z.mean()\n",
    "print(z)\n",
    "print(z.grad_fn)\n",
    "\n",
    "\n",
    "print(\"\\n ---- z.backward-----\")\n",
    "print(x.grad)\n",
    "z.backward() # dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92a400e8-3cf4-4fc0-935a-868c4f12f9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ x,w with grad option --------\n",
      "tensor([ 0.5146, -0.1458,  0.1280], requires_grad=True)\n",
      "tensor(2.7758, grad_fn=<MeanBackward0>)\n",
      "None\n",
      "<MeanBackward0 object at 0x78a15270d150>\n",
      "\n",
      "------ z which is a function of y and hence x,w-------\n",
      "tensor(23.1157, grad_fn=<MulBackward0>)\n",
      "<MulBackward0 object at 0x78a15270d150>\n",
      "\n",
      "-----------\n",
      "tensor(23.1157, grad_fn=<MeanBackward0>)\n",
      "<MeanBackward0 object at 0x78a15270d150>\n",
      "\n",
      " ---- y.backward and z.backward----\n",
      "None\n",
      "tensor([0.3333, 0.3333, 0.3333])\n",
      "tensor([5.8850, 5.8850, 5.8850])\n"
     ]
    }
   ],
   "source": [
    "print(\"------ x,w with grad option --------\")\n",
    "# When requires_grad = True , it is tracked on a computational graph\n",
    "x1 = torch.randn(3, requires_grad = True)\n",
    "w1 = torch.randn(3, requires_grad = True)\n",
    "y1 = 2*w1 + x1 + 2\n",
    "y1 = y1.mean()\n",
    "print(x1)\n",
    "print(y1)\n",
    "print(x1.grad_fn)\n",
    "print(y1.grad_fn)\n",
    "\n",
    "print(\"\\n------ z which is a function of y and hence x,w-------\")\n",
    "z1 = y1*y1*3\n",
    "print(z1)\n",
    "print(z1.grad_fn)\n",
    "\n",
    "print(\"\\n-----------\")\n",
    "z1 = z1.mean()\n",
    "print(z1)\n",
    "print(z1.grad_fn)\n",
    "\n",
    "print(\"\\n ---- y.backward and z.backward----\")\n",
    "\n",
    "print(x1.grad)\n",
    "# retain graph is needed so that intermediary results are stored\n",
    "y1.backward(retain_graph=True) # dy/dx and dy/dw ?? \n",
    "print(x1.grad)\n",
    "z1.backward(retain_graph=True) # dz/dx and dy/dw ??\n",
    "print(x1.grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
