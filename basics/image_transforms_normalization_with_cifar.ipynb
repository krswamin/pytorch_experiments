{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9db8674-a9a1-4ec9-a1b8-4d26c165abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c76ee64-534b-4d0a-adbd-166e885b94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_dir = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909e3a1-c94c-4b86-87d7-0c252106f980",
   "metadata": {},
   "source": [
    "# Exercise 1 Mean & Std deviation of CIFAR datasets (Before and after Normalization)\n",
    "## Exercise 1.1: Calculate it on the Dataset (not on the data loader)\n",
    "Notice that there is no change before and after normalization. This is because the transform (transforms.toTensor and the Normalization) are not applied on the dataset. This only takes effect after the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d21389-cbdf-4fa0-b89e-d836fb11b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Device Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
    "train_dataset1 = torchvision.datasets.CIFAR10(root=root_data_dir, train=True,\n",
    "                                        download=True, transform=transforms.ToTensor())\n",
    "\n",
    "test_dataset1 = torchvision.datasets.CIFAR10(root=root_data_dir, train=False,\n",
    "                                       download=True, transform=transforms.ToTensor())\n",
    "# CIFAR 10\n",
    "print(\"BEFORE TRANSFORMATION(Before Normalization): Data mean and std on the DATASET\")\n",
    "print(\"train_dataset: details (before transformation\")\n",
    "print(f'train_dataset1 size     :{list(train_dataset1.data.shape)}')\n",
    "print(f'train_dataset1 mean     :{train_dataset1.data.mean(axis = (0,1,2))}')\n",
    "print(f'train_dataset1 mean/255 :{train_dataset1.data.mean(axis = (0,1,2))/255}') \n",
    "print(f'train_dataset1 std-dev  :{train_dataset1.data.std(axis = (0,1,2))}')\n",
    "print(f'train_dataset1 std-dev/255:{train_dataset1.data.std(axis = (0,1,2))/255}') \n",
    "\n",
    "print(\"\\ntest_dataset: details (before transformation\")\n",
    "print(f'test_dataset1 size     :{list(test_dataset1.data.shape)}')\n",
    "print(f'test_dataset1 mean     :{test_dataset1.data.mean(axis = (0,1,2))}')\n",
    "print(f'test_dataset1 mean/255 :{test_dataset1.data.mean(axis = (0,1,2))/255}') \n",
    "print(f'test_dataset1 std-dev  :{test_dataset1.data.std(axis = (0,1,2))}')\n",
    "print(f'test_dataset1 std-dev/255:{test_dataset1.data.std(axis = (0,1,2))/255}') \n",
    "\n",
    "# dataset has PILImage images of range [0.1]\n",
    "# We transform them to i) Tensors ii) normalized in the range [0,1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean= (125.30691805,122.95039414,113.86538318), # These normalization values are wrong, more on that later\n",
    "                          std=(62.99321928,62.08870764,66.70489964))])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=root_data_dir, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=root_data_dir, train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "\n",
    "# CIFAR 10\n",
    "print(\"\\n\\nAFTER TRANSFORMATION(After Normalization): Data mean and std on the DATASET (not dataloader) \\\n",
    "       \\nNotice that there is no change. \\\n",
    "       \\nThis is because the  Transforms (and hence the normalization) does not take effect until the data loader is called\")\n",
    "print(\"train_dataset: details (after transformation)\")\n",
    "print(f'train_dataset size     :{list(train_dataset.data.shape)}')\n",
    "print(f'train_dataset mean     :{train_dataset.data.mean(axis = (0,1,2))}')\n",
    "print(f'train_dataset mean/255 :{train_dataset.data.mean(axis = (0,1,2))/255}') \n",
    "print(f'train_dataset std-dev  :{train_dataset.data.std(axis = (0,1,2))}')\n",
    "print(f'train_dataset std-dev/255:{train_dataset.data.std(axis = (0,1,2))/255}') \n",
    "\n",
    "print(\"\\ntest_dataset: details (after transformation\")\n",
    "print(f'test_dataset size     :{list(test_dataset.data.shape)}')\n",
    "print(f'test_dataset mean     :{test_dataset.data.mean(axis = (0,1,2))}')\n",
    "print(f'test_dataset mean/255 :{test_dataset.data.mean(axis = (0,1,2))/255}') \n",
    "print(f'test_dataset std-dev  :{test_dataset.data.std(axis = (0,1,2))}')\n",
    "print(f'test_dataset std-dev/255:{test_dataset.data.std(axis = (0,1,2))/255}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76105bd9-1d79-46fe-b63e-e9467d5c252c",
   "metadata": {},
   "source": [
    "# Exercie 1: Mean & Std deviation of CIFAR datasets (Before and after Normalization)\n",
    "## Exercise 1.2: Calculate it on the DATALOADER (not on the dataset)\n",
    "\n",
    "\n",
    "### Transforms and Dataloader\n",
    "In PyTorch, transforms are applied to data when the data is accessed through a DataLoader. Specifically, the transforms specified in the Dataset are applied within the __getitem__ method of the Dataset class. When a DataLoader iterates through the Dataset, it calls __getitem__ for each index, and it is at this point that the transforms are applied to the data before it is returned.\n",
    "If you are using a custom Dataset, you would typically include the transform application within your __getitem__ method:\n",
    "Python\n",
    "\n",
    "Hence the effect of Normalization can only be seen when the mean and standard deviation are calculated on the dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc2ba85-4358-4779-ad62-fd725b36b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Device Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
    "train_dataset1 = torchvision.datasets.CIFAR10(root=root_data_dir, \n",
    "                                              train=True,\n",
    "                                              download=True,\n",
    "                                              transform=transforms.ToTensor())\n",
    "\n",
    "test_dataset1 = torchvision.datasets.CIFAR10(root=root_data_dir,\n",
    "                                             train=False,\n",
    "                                             download=True, transform=transforms.ToTensor())\n",
    "# Create a dataloader\n",
    "trainloader1 = torch.utils.data.DataLoader(train_dataset1, batch_size=100, shuffle=False)\n",
    "\n",
    "# Calculate the mean and standard deviation\n",
    "mean = 0\n",
    "std = 0\n",
    "for i, data in enumerate(trainloader1, 0):\n",
    "    # Get the input data\n",
    "    inputs, labels = data\n",
    "\n",
    "    # Calculate the mean and standard deviation for each batch\n",
    "    mean += torch.mean(inputs, dim=[0, 2, 3])\n",
    "    std += torch.std(inputs, dim=[0, 2, 3])\n",
    "\n",
    "# Calculate the overall mean and standard deviation\n",
    "mean /= i + 1\n",
    "std /= i + 1\n",
    "\n",
    "print(\"Mean(before normalization):\", mean)\n",
    "print(\"Standard Deviation(before normalization):\", std)\n",
    "\n",
    "# dataset has PILImage images of range [0.1]\n",
    "# We transform them to i) Tensors ii) normalized in the range [0,1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean= (125.30691805,122.95039414,113.86538318), # These normalization values are wrong, more on that later\n",
    "                          std=(62.99321928,62.08870764,66.70489964))])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=root_data_dir, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=root_data_dir, train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "\n",
    "# Create a dataloader\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "# Calculate the mean and standard deviation\n",
    "mean = 0\n",
    "std = 0\n",
    "for i, data in enumerate(trainloader, 0):\n",
    "    # Get the input data\n",
    "    inputs, labels = data\n",
    "\n",
    "    # Calculate the mean and standard deviation for each batch\n",
    "    mean += torch.mean(inputs, dim=[0, 2, 3])\n",
    "    std += torch.std(inputs, dim=[0, 2, 3])\n",
    "\n",
    "# Calculate the overall mean and standard deviation\n",
    "mean /= i + 1\n",
    "std /= i + 1\n",
    "\n",
    "print(\"\\n Mean(after normalization):\\n\", mean)\n",
    "print(\"Standard Deviation(after normalization: \\n the normalization values used could be wrong !!!!!. More on that later):\\n\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ceaca-a835-424d-8f55-16670883a03f",
   "metadata": {},
   "source": [
    "# Exercise 2 : Transforms, Mean and Standard Deviation\n",
    "https://stackoverflow.com/questions/66678052/how-to-calculate-the-mean-and-the-std-of-cifar10-data \\\n",
    "\n",
    "\n",
    "## Exercise 2.1: Just using Min and Max. No dataloader, no inline iteration , no for loop\n",
    "Notice that in the below example you have to divide the dataset by 255 in order to get the same results as section 2.2, 2.3 and 2.4\n",
    "This is because in sections 2.2- 2.4, the transorm= transforms.ToTensor() is applied (while this is not applied in section 2.1)\n",
    "\n",
    "### What does transforms.ToTensor() do\n",
    "https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html \\\n",
    "transforms.ToTensor assumes that a PIL Image in the range [0,255] is being passed. It divides the image by 255 and normalizes in the range [0,1] \\\n",
    "Note: \\\n",
    "1) the max value in the dataset could be 200 and not 255, in which case the max value in the dataset would be 200/255 = 0.7843. So another subsequent normalization might be necessary to truly make the range of the data [0,1] (and this entire notebook is about that LOL) \\\n",
    "2) When composing transforms. the order is i) transforms.ToTensor ii) Normalization \\\n",
    "   Example: transform = transforms.Compose( \\\n",
    "   [transforms.ToTensor(), \\\n",
    "   transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\\\n",
    "                        std=(0.2466, 0.2431, 0.2610))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5c53a-cfe5-4eb3-9138-aed8c95ddeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  torchvision import datasets\n",
    "\n",
    "cifar_trainset = datasets.CIFAR10(root=root_data_dir, train=True, download=True  )\n",
    "data = cifar_trainset.data / 255 # data is numpy array\n",
    "\n",
    "mean  = cifar_trainset.data.mean(axis = (0,1,2)) \n",
    "std   = cifar_trainset.data.std(axis = (0,1,2))\n",
    "print(\"Mean and Standard Deviation before dividing by 255\")\n",
    "print(\"Mean:\", mean)\n",
    "print(\"STD :\", std)\n",
    "\n",
    "mean = data.mean(axis = (0,1,2)) \n",
    "std  = data.std(axis = (0,1,2))\n",
    " #Mean : [0.491 0.482 0.446]   STD: [0.247 0.243 0.261]\n",
    "print(\"\\nMean and Standard Deviation After dividing by 255\")\n",
    "print(\"Mean:\", mean)\n",
    "print(\"STD :\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08aa6e2-4a78-451b-8dff-ff114503123e",
   "metadata": {},
   "source": [
    "# Exercise 2 : Transforms, Mean and Standard Deviation\n",
    "https://stackoverflow.com/questions/66678052/how-to-calculate-the-mean-and-the-std-of-cifar10-data \\\n",
    "## Exercise 2.2 : Without an explicit data loader: For Loop\n",
    "Notice that in the below example there is no explicit data loader despite that the transorm= transforms.ToTensor() is applied\n",
    "\n",
    "This is because when iterating over a PyTorch Dataset in a for loop or using it with a DataLoader, the __getitem__ method is called for each index accessed during the iteration. This is how PyTorch retrieves individual data samples from the dataset.\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d7b32a-4b8a-4404-903f-8a2de1b85473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root=root_data_dir, train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Initialize variables for mean and standard deviation calculation\n",
    "total_sum = torch.zeros(3)\n",
    "total_squared_sum = torch.zeros(3)\n",
    "num_batches = len(trainset)\n",
    "count = 0\n",
    "\n",
    "# Iterate through the dataset to calculate the sum of pixel values and the sum of squared pixel values\n",
    "for data, _ in trainset:\n",
    "    total_sum += torch.sum(data, dim=[1, 2])\n",
    "    total_squared_sum += torch.sum(data ** 2, dim=[1, 2])\n",
    "    count += data.shape[1] * data.shape[2]\n",
    "\n",
    "# Calculate the mean for each channel\n",
    "mean = total_sum / count\n",
    "\n",
    "# Calculate the standard deviation for each channel\n",
    "std = torch.sqrt((total_squared_sum / count) - (mean ** 2))\n",
    "\n",
    "print(\"Mean and Standard Deviation Using For Loop\")\n",
    "print(\"Mean:\", mean)\n",
    "print(\"STD :\", std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c3a0f-82cd-4af1-950a-2f0eef359970",
   "metadata": {},
   "source": [
    "# Exercise 2 : Transforms, Mean and Standard Deviation\n",
    "https://stackoverflow.com/questions/66678052/how-to-calculate-the-mean-and-the-std-of-cifar10-data \\\n",
    "## Exercise 2.3 : Without an explicit data loader: Inline iteration\n",
    "Similar to for loop\n",
    "Notice that in the below example there is no explicit data loader despite that the transorm= transforms.ToTensor() is applied\n",
    "\n",
    "This is because when you iterate directly over a Dataset object, Python implicitly calls the __iter__ method, which then relies on __getitem__ to fetch the data. The iteration continues until an IndexError is raised, signaling the end of the dataset. \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b107734-4e19-4836-adfe-a1d1a16d03e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "cifar_trainset = datasets.CIFAR10(root=root_data_dir, train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "imgs = [item[0] for item in cifar_trainset] # item[0] and item[1] are image and its label\n",
    "imgs = torch.stack(imgs, dim=0).numpy()\n",
    "\n",
    "# calculate mean over each channel (r,g,b)\n",
    "mean_r = imgs[:,0,:,:].mean()\n",
    "mean_g = imgs[:,1,:,:].mean()\n",
    "mean_b = imgs[:,2,:,:].mean()\n",
    "\n",
    "# calculate std over each channel (r,g,b)\n",
    "std_r = imgs[:,0,:,:].std()\n",
    "std_g = imgs[:,1,:,:].std()\n",
    "std_b = imgs[:,2,:,:].std()\n",
    "\n",
    "print(\"Mean and Standard Deviation: Inline Iteration\")\n",
    "print(\"Mean :\",mean_r,mean_g,mean_b)\n",
    "print(\"STD  :\", std_r,std_g,std_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ae5e8d-3159-48e5-b1a9-ec67afbc1217",
   "metadata": {},
   "source": [
    "# Exercise 2 : Transforms, Mean and Standard Deviation\n",
    "https://stackoverflow.com/questions/66678052/how-to-calculate-the-mean-and-the-std-of-cifar10-data \\\n",
    "## Exercise 2.4 : With a data loader\n",
    "### Difference between for loop /iteration vs dataloader for loading data\n",
    "#### For Loop Iteration \n",
    "A basic for loop directly accesses elements from the dataset one by one. This approach is straightforward for small datasets but becomes inefficient for large datasets due to the lack of parallel processing and batching.\\\n",
    "\n",
    "#### DataLoader Iteration\n",
    "When using a DataLoader, it handles the iteration process, including batching, shuffling, and parallel loading. This significantly improves efficiency, especially for large datasets, by utilizing multiple CPU cores and reducing data loading time. The dataloader internally calls __getitem__ to retrieve the data samples based on the sampler it uses. The dataloader uses a Sampler to generate indices, and for each index, the __getitem__ method is called to retrieve the corresponding data sample. This happens for every item in the dataset during a full iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c214e-e417-45c5-91b2-d1d2fe57e30a",
   "metadata": {},
   "source": [
    "### Exercise 2.4.1: How NOT TO use a dataloader !!!! Lol !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1080a23-e06c-432f-bf8f-236a41397719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=root_data_dir, train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Create a dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "def get_mean_std(trainLoader):\n",
    "    imgs = None\n",
    "    for batch in trainLoader:\n",
    "        image_batch = batch[0]\n",
    "        if imgs is None:\n",
    "            imgs = image_batch.cpu()\n",
    "        else:\n",
    "            imgs = torch.cat([imgs, image_batch.cpu()], dim=0)\n",
    "    imgs = imgs.numpy()\n",
    "    \n",
    "    # calculate mean over each channel (r,g,b)\n",
    "    mean_r = imgs[:,0,:,:].mean()\n",
    "    mean_g = imgs[:,1,:,:].mean()\n",
    "    mean_b = imgs[:,2,:,:].mean()\n",
    "\n",
    "    # calculate std over each channel (r,g,b)\n",
    "    std_r = imgs[:,0,:,:].std()\n",
    "    std_g = imgs[:,1,:,:].std()\n",
    "    std_b = imgs[:,2,:,:].std()\n",
    "\n",
    "\n",
    "    print(\"Mean and Standard Deviation: Data Loader \")\n",
    "    print(\"This is so slow , dont use this method\")\n",
    "    print(\"Mean :\",mean_r,mean_g,mean_b)\n",
    "    print(\"STD  :\", std_r,std_g,std_b)\n",
    "\n",
    "get_mean_std(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38baef-ef99-41d3-a034-536e29136534",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 2.4.2: Correct way TO use a dataloader !!!! Lol !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d2ceca-377f-4230-8ac5-ff1116fdf805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the transform to convert images to tensors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load the CIFAR10 training dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root=root_data_dir, train=True, download=True, transform=transform)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(trainset, batch_size=10000, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize variables to store the sum and squared sum of pixel values\n",
    "channels_sum, channels_squared_sum = 0, 0\n",
    "num_batches = 0\n",
    "\n",
    "# Iterate through the DataLoader to calculate the sum and squared sum\n",
    "for data, _ in dataloader:\n",
    "    channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
    "    channels_squared_sum += torch.mean(data**2, dim=[0, 2, 3])\n",
    "    num_batches += 1\n",
    "    \n",
    "# Calculate the mean and standard deviation\n",
    "mean = channels_sum / num_batches\n",
    "std = (channels_squared_sum / num_batches - mean**2) ** 0.5\n",
    "\n",
    "print(\"Mean and Standard Deviation: Data Loader : CORRECT METHOD\")\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"STD : {std}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
